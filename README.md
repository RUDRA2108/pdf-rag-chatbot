# ğŸ“š PDF RAG Chatbot

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue)](https://www.python.org/)
[![Gradio](https://img.shields.io/badge/UI-Gradio-orange)](https://www.gradio.app/)
[![ChromaDB](https://img.shields.io/badge/VectorDB-Chroma-green)](https://www.trychroma.com/)
[![Ollama](https://img.shields.io/badge/LLM-Ollama-black)](https://ollama.ai/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

> Chat with your PDFs using **Retrieval-Augmented Generation (RAG)** â€“ **completely local**.  
> Everything runs on your own machine using [Ollama](https://ollama.ai/). No cloud, no external API calls.  
> Extracts **text, tables, and images** (with captions) from long documents, indexes them, and lets you query them locally with an LLM.

---

## âœ¨ Features
- ğŸ–¥ï¸ **Runs fully offline** â€“ all models run locally via Ollama.  
- ğŸ“‘ **Multi-PDF ingestion** â€“ process thousands of pages at once.  
- ğŸ” **RAG-powered search** â€“ retrieve relevant context before answering.  
- ğŸ“Š **Table & image support** â€“ tables are converted to Markdown, images captioned with a vision model.  
- ğŸ’¬ **Interactive chatbot UI** built with Gradio.  
- ğŸ“¥ **Export chat history to PDF**.  
- âš¡ **Local-first** â€“ your data never leaves your computer.  

---

## ğŸš€ Getting Started

### 1. Clone the repository
```bash
git clone https://github.com/RUDRA2108/pdf-rag-chatbot.git
cd pdf-rag-chatbot
```

### 2. Create & activate virtual environment (recommended)
```bash
python3 -m venv venv
source venv/bin/activate   # On Linux/Mac
venv\Scripts\activate      # On Windows
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Setup environment variables
Copy the example file and edit if needed:
```bash
cp .env.example .env
```

Default `.env` values:
```env
OLLAMA_HOST=http://localhost:11434
EMBED_MODEL=nomic-embed-text
LLM_MODEL=llama3:8b
VISION_MODEL=llava
CHROMA_DIR=vectorstore
```

âš ï¸ Make sure [Ollama](https://ollama.ai/) is installed and running locally on your machine.  
No internet connection is required once models are downloaded.

---

## ğŸ“‚ Usage

### Step 1: Ingest PDFs
Place your PDF files inside the `Documents/` folder, then run:
```bash
python ingest.py --recursive
```
This will parse PDFs â†’ extract text, tables, images â†’ save results into `output/`.

### Step 2: Launch the chatbot
```bash
python app.py
```
Open your browser at **http://localhost:7860**.  
You can now chat with your documents **fully locally**.

### Step 3: Export chat
Click the **ğŸ“¥ Download Chat as PDF** button to save your conversation.

---

## ğŸ›  Project Structure
```
pdf-rag-chatbot/
â”œâ”€â”€ app.py           # Chatbot app (Gradio + RAG)
â”œâ”€â”€ ingest.py        # PDF ingestion pipeline
â”œâ”€â”€ Documents/       # Place your PDFs here
â”œâ”€â”€ output/          # Processed PDFs (Markdown + JSON + images/tables)
â”œâ”€â”€ vectorstore/     # ChromaDB storage
â”œâ”€â”€ .env.example     # Example environment file
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ¤ Contributing
Pull requests are welcome!  
For major changes, please open an issue first to discuss what youâ€™d like to change.

---

## ğŸ“œ License
This project is licensed under the [MIT License](LICENSE).
